(pytorch) ubuntu@ip-172-31-37-253:~/data$ accelerate launch train_llama_local_v5.py --output_dir "./llama_overfitting_test_run3" --vocab_size 128256 --hidden_size 256 --intermediate_size 512 --num_hidden_layers 2 --num_attention_heads 4 --num_key_value_heads 2 --max_position_embeddings 131072 --rms_norm_eps 1e-5 --rope_theta 500000.0 --rope_scaling_json '{"type": "llama3", "factor": 32.0, "original_max_position_embeddings": 8192, "low_freq_factor": 1.0, "high_freq_factor": 4.0}' --text_column "text" --sequence_length 512 --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --learning_rate 1e-4 --weight_decay 0.0 --num_train_epochs 3 --max_train_steps 240000 --num_warmup_steps 100 --seed 42 --save_steps 20000 --generation_steps 10000 --generation_prompt "The meaning of life is" --max_new_tokens 100 --mixed_precision "bf16" --use_gradient_checkpointing True --use_8bit_optimizer True --torch_compile True --report_to "tensorboard"
[2025-06-01 08:21:05,462] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/pytorch/lib/python3.12/site-packages/accelerate/utils/dataclasses.py:1259: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.
  warnings.warn("DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.")
[2025-06-01 08:21:12,844] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-01 08:21:14,221] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-06-01 08:21:14,221] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
06/01/2025 08:21:14 - INFO - __main__ - Distributed environment: DistributedType.DEEPSPEED  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 0, 'offload_optimizer': {'device': 'none', 'nvme_path': None}, 'offload_param': {'device': 'none', 'nvme_path': None}, 'stage3_gather_16bit_weights_on_model_save': False}, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

[rank0]:[W601 08:21:14.448366490 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
06/01/2025 08:21:14 - INFO - __main__ - Initializing model from scratch...
06/01/2025 08:21:14 - INFO - __main__ - Applying RoPE scaling configuration: {'type': 'llama3', 'factor': 32.0, 'original_max_position_embeddings': 8192, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0}
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

06/01/2025 08:21:15 - INFO - __main__ - Model parameters: 34,014,464
06/01/2025 08:21:15 - INFO - __main__ - Gradient checkpointing enabled.
06/01/2025 08:21:15 - INFO - __main__ - Loading tokenizer: meta-llama/Llama-3.2-1B
loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
06/01/2025 08:21:16 - INFO - __main__ - Loading dataset: None
06/01/2025 08:21:16 - INFO - __main__ - No --dataset_name provided. Using internal prepare_combined_dataset_streaming().
06/01/2025 08:21:16 - INFO - __main__ - üåê Streaming Skylion007/openwebtext (limit 30000)...
06/01/2025 08:21:19 - INFO - __main__ - üåê Streaming allenai/c4 (limit 30000)...
Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1024/1024 [00:04<00:00, 250.66it/s]
Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1024/1024 [00:02<00:00, 356.61it/s]
06/01/2025 08:21:34 - INFO - __main__ - üåê Streaming togethercomputer/RedPajama-Data-1T (limit 30000)...
06/01/2025 08:21:38 - INFO - __main__ - üåê Streaming ai4bharat/sangraha (limit 30000)...
Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 111.61it/s]
06/01/2025 08:21:42 - INFO - __main__ - üåê Streaming zicsx/mC4-Hindi-Cleaned-3.0 (limit 30000)...
Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [00:00<00:00, 66.87it/s]
Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [00:00<00:00, 246723.76it/s]
06/01/2025 08:21:46 - INFO - __main__ - üåê Streaming PleIAs/common_corpus (limit 30000)...
Repo card metadata block was not found. Setting CardData to empty.
06/01/2025 08:21:46 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10011/10011 [00:01<00:00, 8216.16it/s]
06/01/2025 08:22:12 - INFO - __main__ - ‚úÖ Combined streaming dataset ready.
06/01/2025 08:22:12 - INFO - __main__ - Internal dataset preparation complete. Effective streaming: True
06/01/2025 08:22:12 - INFO - __main__ - Processing dataset as a stream.
06/01/2025 08:22:13 - INFO - __main__ - Mapping dataset to select/create 'text'. Original columns found: ['text', 'timestamp', 'url', 'meta', 'red_pajama_subset', 'doc_id', 'type', 'identifier', 'collection', 'open_type', 'license', 'date', 'title', 'creator', 'language', 'language_type', 'word_count', 'token_count', '__index_level_0__']. Columns to remove: ['timestamp', 'url', 'meta', 'red_pajama_subset', 'doc_id', 'type', 'identifier', 'collection', 'open_type', 'license', 'date', 'title', 'creator', 'language', 'language_type', 'word_count', 'token_count', '__index_level_0__'].
06/01/2025 08:22:13 - INFO - __main__ - Using streaming dataset. Tokenization will be applied on-the-fly.
06/01/2025 08:22:13 - INFO - __main__ - Applying tokenization on column 'text'.
06/01/2025 08:22:13 - INFO - __main__ - Using streaming dataset. Training will proceed for args.max_train_steps.
06/01/2025 08:22:13 - INFO - __main__ - Using 8-bit AdamW optimizer.
06/01/2025 08:22:13 - INFO - __main__ - Using SchedulerType.COSINE scheduler with 100 warmup steps.
[2025-06-01 08:22:13,148] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-06-01 08:22:13,148] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1
[2025-06-01 08:22:13,236] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-06-01 08:22:13,237] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-06-01 08:22:13,237] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-06-01 08:22:13,237] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW8bit
[2025-06-01 08:22:13,237] [INFO] [logging.py:107:log_dist] [Rank 0] Creating BF16 optimizer
[2025-06-01 08:22:13,416] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2025-06-01 08:22:13,416] [INFO] [utils.py:782:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.07 GB         Max_CA 0 GB 
[2025-06-01 08:22:13,416] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.06 GB, percent = 48.6%
[2025-06-01 08:22:13,594] [INFO] [utils.py:781:see_memory_usage] before initializing group 0
[2025-06-01 08:22:13,594] [INFO] [utils.py:782:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.07 GB         Max_CA 0 GB 
[2025-06-01 08:22:13,594] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.06 GB, percent = 48.6%
[2025-06-01 08:22:13,781] [INFO] [utils.py:781:see_memory_usage] after initializing group 0
[2025-06-01 08:22:13,781] [INFO] [utils.py:782:see_memory_usage] MA 0.32 GB         Max_MA 0.32 GB         CA 0.45 GB         Max_CA 0 GB 
[2025-06-01 08:22:13,782] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.06 GB, percent = 48.6%
[2025-06-01 08:22:13,963] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2025-06-01 08:22:13,963] [INFO] [utils.py:782:see_memory_usage] MA 0.32 GB         Max_MA 0.32 GB         CA 0.45 GB         Max_CA 0 GB 
[2025-06-01 08:22:13,963] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.05 GB, percent = 48.6%
[2025-06-01 08:22:13,964] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer
[2025-06-01 08:22:13,964] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-06-01 08:22:13,964] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-06-01 08:22:13,964] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-06-01 08:22:13,964] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-06-01 08:22:13,964] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-06-01 08:22:13,964] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-06-01 08:22:13,964] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-06-01 08:22:13,964] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x70bc1c466750>
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-06-01 08:22:13,965] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-06-01 08:22:13,966] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   train_batch_size ............. 1
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-06-01 08:22:13,967] [INFO] [config.py:1007:print]   world_size ................... 1
[2025-06-01 08:22:13,968] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-06-01 08:22:13,968] [INFO] [config.py:1007:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-06-01 08:22:13,968] [INFO] [config.py:1007:print]   zero_enabled ................. False
[2025-06-01 08:22:13,968] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-06-01 08:22:13,968] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 0
[2025-06-01 08:22:13,968] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 1, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 0, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
06/01/2025 08:22:13 - INFO - __main__ - Attempting to compile the model with torch.compile...
06/01/2025 08:22:14 - INFO - __main__ - Model compiled successfully.
06/01/2025 08:22:14 - INFO - __main__ - ***** Running training *****
06/01/2025 08:22:14 - INFO - __main__ -   Num examples = N/A (streaming dataset)
06/01/2025 08:22:14 - INFO - __main__ -   Num Epochs = N/A (streaming, will run for 240000 steps)
06/01/2025 08:22:14 - INFO - __main__ -   Instantaneous batch size per device = 1
06/01/2025 08:22:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1
06/01/2025 08:22:14 - INFO - __main__ -   Gradient Accumulation steps = 1
06/01/2025 08:22:14 - INFO - __main__ -   Total optimization steps = 240000
06/01/2025 08:22:14 - INFO - __main__ - No checkpoint specified for resumption. Starting training from scratch.
06/01/2025 08:22:14 - INFO - __main__ - ***** Running training *****
06/01/2025 08:22:14 - INFO - __main__ -   Initial optimizer steps completed (if resuming): 0
06/01/2025 08:22:14 - INFO - __main__ -   Target total optimizer steps for this run: 240000
  0%|                                                                                                                                                   | 0/240000 [00:00<?, ?it/s]skipping cudagraphs due to disabling cudagraphs due to incompatible op run_and_save_rng_state
Epoch 1 Step 1 LR: 1.00e-06 Loss: 11.7807:   0%|                                                                                           | 1/240000 [00:24<1658:55:10, 24.88s/it]skipping cudagraphs due to disabling cudagraphs due to incompatible op run_and_save_rng_state
Epoch 1 Step 7 LR: 7.00e-06 Loss: 11.8010:   0%|                                                                                            | 7/240000 [00:35<214:18:42,  3.21s/it]skipping cudagraphs due to disabling cudagraphs due to incompatible op run_and_save_rng_state
Epoch 1 Step 10 LR: 1.00e-05 Loss: 11.7957:   0%|                                                                                          | 10/240000 [00:48<214:18:32,  3.21s/it]skipping cudagraphs due to disabling cudagraphs due to incompatible op run_and_save_rng_state
Epoch 1 Step 10000 LR: 9.96e-05 Loss: 6.3588:   4%|‚ñà‚ñà‚ñà‚ñã                                                                                     | 10000/240000 [04:06<57:46, 66.35it/s]06/01/2025 08:26:20 - INFO - __main__ - 
--- Generating sample at step 10000 ---
06/01/2025 08:26:21 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:26:21 - INFO - __main__ - Generated: The meaning of life is the first time to the best of the U.S. A year, and the most of a 2013-2017, and it‚Äôs a lot of the season.

The most of the 2009, and the first thing in the 2016.

The ‚ÄúWe would be a new time. I was the most of the U.S. I think, the first time to be a few years, and it is a long of a good. It‚Äôs going, it's a few

Epoch 1 Step 20000 LR: 9.83e-05 Loss: 5.9103:   8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                 | 20000/240000 [07:13<55:44, 65.78it/s]06/01/2025 08:29:27 - INFO - __main__ - 
--- Generating sample at step 20000 ---
06/01/2025 08:29:28 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:29:28 - INFO - __main__ - Generated: The meaning of life is the most of the first, the first two years. The best way to the most important way to the most recent years, it‚Äôs most important to make them the best. But the way I don‚Äôt do you, it‚Äôs not the great reason for you. The great work is an important way you can‚Äôt be able to be the best of my favorite and a way you can be sure to do with you, it‚Äôs most important to your own. It‚Äôs one thing to be in the

06/01/2025 08:29:28 - INFO - accelerate.accelerator - Saving current state to ./llama_overfitting_test_run3/checkpoint-20000
06/01/2025 08:29:28 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2025-06-01 08:29:28,200] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2025-06-01 08:29:28,201] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model/mp_rank_00_model_states.pt
[2025-06-01 08:29:28,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model/mp_rank_00_model_states.pt...
[2025-06-01 08:29:28,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model/mp_rank_00_model_states.pt.
[2025-06-01 08:29:28,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-01 08:29:28,671] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-01 08:29:28,671] [INFO] [engine.py:3701:_save_zero_checkpoint] bf16_zero checkpoint saved ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-01 08:29:28,671] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
06/01/2025 08:29:28 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir ./llama_overfitting_test_run3/checkpoint-20000/pytorch_model
06/01/2025 08:29:28 - INFO - accelerate.checkpointing - Scheduler state saved in llama_overfitting_test_run3/checkpoint-20000/scheduler.bin
06/01/2025 08:29:28 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in llama_overfitting_test_run3/checkpoint-20000/sampler.bin
06/01/2025 08:29:28 - INFO - accelerate.checkpointing - Random states saved in llama_overfitting_test_run3/checkpoint-20000/random_states_0.pkl
Configuration saved in ./llama_overfitting_test_run3/checkpoint-20000/config.json
Configuration saved in ./llama_overfitting_test_run3/checkpoint-20000/generation_config.json
Model weights saved in ./llama_overfitting_test_run3/checkpoint-20000/model.safetensors
tokenizer config file saved in ./llama_overfitting_test_run3/checkpoint-20000/tokenizer_config.json
Special tokens file saved in ./llama_overfitting_test_run3/checkpoint-20000/special_tokens_map.json
06/01/2025 08:29:29 - INFO - __main__ - Saved checkpoint to ./llama_overfitting_test_run3/checkpoint-20000
Epoch 1 Step 30000 LR: 9.62e-05 Loss: 5.6762:  12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                             | 30000/240000 [10:27<52:21, 66.86it/s]06/01/2025 08:32:41 - INFO - __main__ - 
--- Generating sample at step 30000 ---
06/01/2025 08:32:41 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:32:41 - INFO - __main__ - Generated: The meaning of life is not the most popular thing that I‚Äôm going to get the best and it‚Äôs just just a little bit of it, which has been in the most popular life.
The most important is the most important, the most important idea was to do with many different kinds of years ago. We also got a lot of time to get to play and make it to be in the past, but it‚Äôs a bit of great things that it‚Äôs not going into the world. This is that it is still going

Epoch 1 Step 40000 LR: 9.33e-05 Loss: 5.5282:  17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                          | 40000/240000 [13:38<49:58, 66.69it/s]06/01/2025 08:35:52 - INFO - __main__ - 
--- Generating sample at step 40000 ---
06/01/2025 08:35:53 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:35:53 - INFO - __main__ - Generated: The meaning of life is the most of the country‚Äôs first day, the most of the world has the largest impact of the world. The most of the most important, and is the world of the most important importance of the country‚Äôs largest most popular, the most popular and modern world of these are not the most of the most common world. And the first time, it is a great part of our lives of the world. This is the most of the world, but you have to take a great time and you‚Äôre

06/01/2025 08:35:53 - INFO - accelerate.accelerator - Saving current state to ./llama_overfitting_test_run3/checkpoint-40000
06/01/2025 08:35:53 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2025-06-01 08:35:53,081] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2025-06-01 08:35:53,082] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model/mp_rank_00_model_states.pt
[2025-06-01 08:35:53,082] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model/mp_rank_00_model_states.pt...
[2025-06-01 08:35:53,205] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model/mp_rank_00_model_states.pt.
[2025-06-01 08:35:53,205] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-01 08:35:53,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-01 08:35:53,548] [INFO] [engine.py:3701:_save_zero_checkpoint] bf16_zero checkpoint saved ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-01 08:35:53,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
06/01/2025 08:35:53 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir ./llama_overfitting_test_run3/checkpoint-40000/pytorch_model
06/01/2025 08:35:53 - INFO - accelerate.checkpointing - Scheduler state saved in llama_overfitting_test_run3/checkpoint-40000/scheduler.bin
06/01/2025 08:35:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in llama_overfitting_test_run3/checkpoint-40000/sampler.bin
06/01/2025 08:35:53 - INFO - accelerate.checkpointing - Random states saved in llama_overfitting_test_run3/checkpoint-40000/random_states_0.pkl
Configuration saved in ./llama_overfitting_test_run3/checkpoint-40000/config.json
Configuration saved in ./llama_overfitting_test_run3/checkpoint-40000/generation_config.json
Model weights saved in ./llama_overfitting_test_run3/checkpoint-40000/model.safetensors
tokenizer config file saved in ./llama_overfitting_test_run3/checkpoint-40000/tokenizer_config.json
Special tokens file saved in ./llama_overfitting_test_run3/checkpoint-40000/special_tokens_map.json
06/01/2025 08:35:53 - INFO - __main__ - Saved checkpoint to ./llama_overfitting_test_run3/checkpoint-40000
Epoch 1 Step 46981 LR: 9.09e-05 Loss: 5.4489:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                       | 46981/240000 [15:47<48:32, 66.26it/s]Epoch 1 Step 50000 LR: 8.97e-05 Loss: 5.4183:  21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 50000/240000 [16:46<47:51, 66.16it/s]06/01/2025 08:39:00 - INFO - __main__ - 
--- Generating sample at step 50000 ---
06/01/2025 08:39:00 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:39:00 - INFO - __main__ - Generated: The meaning of life is the old girl, a mother of my son, and she will be able to her husband and her son, and I have been to have her her father in her life and her father, she said. ‚ÄúI‚Äôm not sure I‚Äôm not happy to see her in the father of her her husband, and I love her. My father is a man who she never worked with the mother and her family. I have been a father, and I love to her her mother, and she is her

Epoch 1 Step 60000 LR: 8.54e-05 Loss: 5.3303:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                  | 60000/240000 [19:52<44:18, 67.70it/s]06/01/2025 08:42:06 - INFO - __main__ - 
--- Generating sample at step 60000 ---
06/01/2025 08:42:07 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:42:07 - INFO - __main__ - Generated: The meaning of life is to be the way to make a good time. I am a good way to make the time to make a great opportunity of my life, but it's the most famous day, it was the first time I was born on the world I found in the first I was the most important in which I was a great one and I had to be so good to be able to do it. It was a nice part of the world I had a great idea to do in the time I had to see

06/01/2025 08:42:07 - INFO - accelerate.accelerator - Saving current state to ./llama_overfitting_test_run3/checkpoint-60000
06/01/2025 08:42:07 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2025-06-01 08:42:07,323] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2025-06-01 08:42:07,324] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model/mp_rank_00_model_states.pt
[2025-06-01 08:42:07,325] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model/mp_rank_00_model_states.pt...
[2025-06-01 08:42:07,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model/mp_rank_00_model_states.pt.
[2025-06-01 08:42:07,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-01 08:42:07,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-01 08:42:07,815] [INFO] [engine.py:3701:_save_zero_checkpoint] bf16_zero checkpoint saved ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-01 08:42:07,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
06/01/2025 08:42:07 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir ./llama_overfitting_test_run3/checkpoint-60000/pytorch_model
06/01/2025 08:42:07 - INFO - accelerate.checkpointing - Scheduler state saved in llama_overfitting_test_run3/checkpoint-60000/scheduler.bin
06/01/2025 08:42:07 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in llama_overfitting_test_run3/checkpoint-60000/sampler.bin
06/01/2025 08:42:07 - INFO - accelerate.checkpointing - Random states saved in llama_overfitting_test_run3/checkpoint-60000/random_states_0.pkl
Configuration saved in ./llama_overfitting_test_run3/checkpoint-60000/config.json
Configuration saved in ./llama_overfitting_test_run3/checkpoint-60000/generation_config.json
Model weights saved in ./llama_overfitting_test_run3/checkpoint-60000/model.safetensors
tokenizer config file saved in ./llama_overfitting_test_run3/checkpoint-60000/tokenizer_config.json
Special tokens file saved in ./llama_overfitting_test_run3/checkpoint-60000/special_tokens_map.json
06/01/2025 08:42:08 - INFO - __main__ - Saved checkpoint to ./llama_overfitting_test_run3/checkpoint-60000
Epoch 1 Step 70000 LR: 8.05e-05 Loss: 5.2615:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                               | 70000/240000 [23:08<42:58, 65.94it/s]06/01/2025 08:45:22 - INFO - __main__ - 
--- Generating sample at step 70000 ---
06/01/2025 08:45:22 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:45:22 - INFO - __main__ - Generated: The meaning of life is an early life is a great experience in the life. The world‚Äôs life, but he‚Äôs a family, and a family, and family, and the life, is the best friend, and is to keep it up with you and it‚Äôs a little girl.
It‚Äôs a great girl, and she‚Äôs a life, but it‚Äôs a lot of things you know. ‚ÄúIt‚Äôs a good time, and a good, and you can make it feel like a child, and I think it

Epoch 1 Step 80000 LR: 7.50e-05 Loss: 5.2056:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                           | 80000/240000 [26:09<38:46, 68.77it/s]06/01/2025 08:48:23 - INFO - __main__ - 
--- Generating sample at step 80000 ---
06/01/2025 08:48:24 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:48:24 - INFO - __main__ - Generated: The meaning of life is the first part of the history of the most popular music. The new design of the most popular, 2-2 of 4,500 and is a 3.1-1, 3 (2) the most 6th century. The 4-1-5, 5.2.2. The 2-2-0.2 (1), which is known as a "0-1-1-2,2.3-0-0 (

06/01/2025 08:48:24 - INFO - accelerate.accelerator - Saving current state to ./llama_overfitting_test_run3/checkpoint-80000
06/01/2025 08:48:24 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2025-06-01 08:48:24,159] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2025-06-01 08:48:24,160] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model/mp_rank_00_model_states.pt
[2025-06-01 08:48:24,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model/mp_rank_00_model_states.pt...
[2025-06-01 08:48:24,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model/mp_rank_00_model_states.pt.
[2025-06-01 08:48:24,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-01 08:48:24,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-01 08:48:24,647] [INFO] [engine.py:3701:_save_zero_checkpoint] bf16_zero checkpoint saved ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-01 08:48:24,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
06/01/2025 08:48:24 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir ./llama_overfitting_test_run3/checkpoint-80000/pytorch_model
06/01/2025 08:48:24 - INFO - accelerate.checkpointing - Scheduler state saved in llama_overfitting_test_run3/checkpoint-80000/scheduler.bin
06/01/2025 08:48:24 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in llama_overfitting_test_run3/checkpoint-80000/sampler.bin
06/01/2025 08:48:24 - INFO - accelerate.checkpointing - Random states saved in llama_overfitting_test_run3/checkpoint-80000/random_states_0.pkl
Configuration saved in ./llama_overfitting_test_run3/checkpoint-80000/config.json
Configuration saved in ./llama_overfitting_test_run3/checkpoint-80000/generation_config.json
Model weights saved in ./llama_overfitting_test_run3/checkpoint-80000/model.safetensors
tokenizer config file saved in ./llama_overfitting_test_run3/checkpoint-80000/tokenizer_config.json
Special tokens file saved in ./llama_overfitting_test_run3/checkpoint-80000/special_tokens_map.json
06/01/2025 08:48:24 - INFO - __main__ - Saved checkpoint to ./llama_overfitting_test_run3/checkpoint-80000
Epoch 1 Step 90000 LR: 6.92e-05 Loss: 5.1567:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                       | 90000/240000 [29:12<35:50, 69.76it/s]06/01/2025 08:51:26 - INFO - __main__ - 
--- Generating sample at step 90000 ---
06/01/2025 08:51:27 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:51:27 - INFO - __main__ - Generated: The meaning of life is a very good and important place in the world.
The most important and important in the world, has been an opportunity to be an excellent, but it is a great opportunity for the life of the life and the future. The most important, the most popular life is that is often a great place for our life. In our life, I will love with us and we will love the best. Our experience is the most important, especially is one of the most beautiful and beautiful. It is very difficult

Epoch 1 Step 100000 LR: 6.30e-05 Loss: 5.1143:  42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                 | 100000/240000 [32:13<6:08:16,  6.34it/s]06/01/2025 08:54:27 - INFO - __main__ - 
--- Generating sample at step 100000 ---
06/01/2025 08:54:27 - INFO - __main__ - Prompt: The meaning of life is
06/01/2025 08:54:27 - INFO - __main__ - Generated: The meaning of life is that people is not the most common of the world.

We believe you are in mind that you are not sure to know what is it?

We know how the best thing in the world that is the most common things we can have to know that the world is very difficult, but we are going to do with the most. We do not think there are something that the people who are going to be on. Our best friend is, we are not happy to hear from our world, but we have

06/01/2025 08:54:27 - INFO - accelerate.accelerator - Saving current state to ./llama_overfitting_test_run3/checkpoint-100000
06/01/2025 08:54:27 - INFO - accelerate.accelerator - Saving DeepSpeed Model and Optimizer
[2025-06-01 08:54:27,700] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint pytorch_model is about to be saved!
[2025-06-01 08:54:27,701] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model/mp_rank_00_model_states.pt
[2025-06-01 08:54:27,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model/mp_rank_00_model_states.pt...
[2025-06-01 08:54:27,824] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model/mp_rank_00_model_states.pt.
[2025-06-01 08:54:27,824] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-06-01 08:54:28,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-06-01 08:54:28,167] [INFO] [engine.py:3701:_save_zero_checkpoint] bf16_zero checkpoint saved ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-06-01 08:54:28,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint pytorch_model is ready now!
06/01/2025 08:54:28 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer saved to output dir ./llama_overfitting_test_run3/checkpoint-100000/pytorch_model
06/01/2025 08:54:28 - INFO - accelerate.checkpointing - Scheduler state saved in llama_overfitting_test_run3/checkpoint-100000/scheduler.bin
06/01/2025 08:54:28 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in llama_overfitting_test_run3/checkpoint-100000/sampler.bin
06/01/2025 08:54:28 - INFO - accelerate.checkpointing - Random states saved in llama_overfitting_test_run3/checkpoint-100000/random_states_0.pkl
Configuration saved in ./llama_overfitting_test_run3/checkpoint-100000/config.json
Configuration saved in ./llama_overfitting_test_run3/checkpoint-100000/generation_config.json
Model weights saved in ./llama_overfitting_test_run3/checkpoint-100000/model.safetensors
tokenizer config file saved in ./llama_overfitting_test_run3/checkpoint-100000/tokenizer_config.json
Special tokens file saved in ./llama_overfitting_test_run3/checkpoint-100000/special_tokens_map.json
06/01/2025 08:54:28 - INFO - __main__ - Saved checkpoint to ./llama_overfitting_test_run3/checkpoint-100000
Epoch 1 Step 105125 LR: 5.97e-05 Loss: 5.0942:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                 | 105125/240000 [33:45<33:31, 67.05it/s]