(pytorch) ubuntu@ip-172-31-9-189:~/data/final_training$ accelerate config
---------------------------------------------------------------------------------------------------------------------------In which compute environment are you running?
This machine                                                                                                               
---------------------------------------------------------------------------------------------------------------------------Which type of machine are you using?                                                                                       
multi-GPU                                                                                                                  
How many different machines will you use (use more than 1 for multi-node training)? [1]: 1                                 
Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: yes                                                                                                                    
Do you wish to optimize your script with torch dynamo?[yes/NO]:yes                                                         
---------------------------------------------------------------------------------------------------------------------------Which dynamo backend would you like to use?                                                                                
cudagraphs                                                                                                                 
Do you want to customize the defaults sent to torch.compile? [yes/NO]: NO                                                  
Do you want to use DeepSpeed? [yes/NO]: yes                                                                                
Do you want to specify a json file to a DeepSpeed config? [yes/NO]: NO                                                     
---------------------------------------------------------------------------------------------------------------------------What should be your DeepSpeed's ZeRO optimization stage?                                                                   
2                                                                                                                          
---------------------------------------------------------------------------------------------------------------------------Where to offload optimizer states?                                                                                         
cpu                                                                                                                        
---------------------------------------------------------------------------------------------------------------------------Where to offload parameters?                                                                                               
cpu                                                                                                                        
How many gradient accumulation steps you're passing in your script? [1]: 1                                                 
Do you want to use gradient clipping? [yes/NO]: yes                                                                        
What is the gradient clipping value? [1.0]: 1.0                                                                            
Do you want to enable `deepspeed.zero.Init` when using ZeRO Stage-3 for constructing massive models? [yes/NO]: yes
Do you want to enable Mixture-of-Experts training (MoE)? [yes/NO]: NO
How many GPU(s) should be used for distributed training? [1]:all
Please enter an integer.
How many GPU(s) should be used for distributed training? [1]:4
---------------------------------------------------------------------------------------------------------------------------Do you wish to use mixed precision?
bf16                                                                                                                       
accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml                           
(pytorch) ubuntu@ip-172-31-9-189:~/data/final_training$ 