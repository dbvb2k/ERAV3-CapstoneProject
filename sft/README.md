# ğŸš€ Ultra-Fast LLaMA 3 Instruct India Travel Assistant

Train LLaMA 3 8B Instruct for India-centric travel assistance with **2x faster training** using Unsloth!

- **Dataset**: 83,598 high-quality examples
- **Speed**: 2x faster than standard training
- **Memory**: 50% less GPU memory usage
- **Quality**: 97.1% excellent examples

## âš¡ Ultra-Fast Training with Unsloth

### ğŸ”§ Step 1: Setup Environment

#### Option A: Google Colab (Recommended for Beginners)
```bash
# Install Unsloth in Colab
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps trl peft accelerate bitsandbytes
```

#### Option B: Local/Server Setup
```bash
# Clone this repository
git clone https://github.com/yourusername/llama3-india-travel-assistant
cd llama3-india-travel-assistant

# Install dependencies
pip install -r training/requirements.txt
```

### ğŸ¤— Step 2: Get Hugging Face Access

1. **Create Hugging Face Account**: https://huggingface.co/join
2. **Get LLaMA 3 Access**: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
3. **Generate Token**: https://huggingface.co/settings/tokens
4. **Login**: `huggingface-cli login` or set `HF_TOKEN`

```bash
# Login to Hugging Face (required for LLaMA 3)
huggingface-cli login
# Enter your token when prompted
```

### ğŸš€ Step 3: Start Ultra-Fast Training

```bash
# One-command training (2-4 hours instead of 8-12!)
python training/train_llama3b_unsloth.py \
  --model_name "unsloth/llama-3-8b-Instruct-bnb-4bit" \
  --train_data data/final/train_llama_format.json \
  --val_data data/final/validation_llama_format.json \
  --output_dir ./travel-assistant-unsloth
```

### ğŸ§ª Step 4: Test Your Model

```bash
# Interactive chat mode
python training/inference_unsloth.py \
  --model_path ./travel-assistant-unsloth/final \
  --interactive

# Test with sample queries
python training/inference_unsloth.py \
  --model_path ./travel-assistant-unsloth/final \
  --test_samples
```

## ğŸ–¥ï¸ GPU Requirements & Configurations

| GPU Model | VRAM | Training Time | Batch Size | Status |
|-----------|------|---------------|------------|--------|
| **RTX 4090** | 24GB | 2-3 hours | 4 | âœ… Optimal |
| **RTX 4080** | 16GB | 3-4 hours | 2 | âœ… Great |
| **RTX 3090** | 24GB | 3-4 hours | 4 | âœ… Great |
| **RTX 3080** | 10GB | 4-5 hours | 2 | âœ… Good |
| **V100** | 16GB | 3-4 hours | 2 | âœ… Good |
| **T4** | 16GB | 5-6 hours | 1 | âœ… Budget |
| **RTX 3070** | 8GB | 6-8 hours | 1 | âš ï¸ Tight |

### ğŸ›ï¸ GPU Memory Optimization

```bash
# For 8GB GPUs (tight memory)
python training/train_llama3b_unsloth.py \
  --model_name "unsloth/llama-3-8b-Instruct-bnb-4bit" \
  --max_seq_length 1024

# For 16GB+ GPUs (optimal)
python training/train_llama3b_unsloth.py \
  --model_name "unsloth/llama-3-8b-Instruct-bnb-4bit" \
  --max_seq_length 2048
```

## ğŸ“Š Dataset Details

- **Size**: 83,598 examples (75,238 train / 8,360 validation)
- **Quality**: 97.1% excellent quality, 6.22/10 average score  
- **Sources**: MultiWOZ (90.4%), MTOP (5.3%), Synthetic (0.2%)
- **Focus**: India-centric travel planning and assistance

## ğŸ¯ Training Performance Comparison

| Method | Training Time | GPU Memory | Quality |
|--------|---------------|------------|---------|
| **Unsloth** | 2-4 hours | 6-12GB | ğŸ† Best |
| Standard LoRA | 8-12 hours | 12-20GB | Good |
| Full Fine-tuning | 24+ hours | 40GB+ | Good |

## ğŸ’¡ Model Capabilities

After training, your model will excel at:
- âœˆï¸ **Flight Booking**: India-specific airlines, routes, pricing
- ğŸ¨ **Hotels**: Budget to luxury, vegetarian-friendly options
- ğŸ›ï¸ **Attractions**: Cultural sites, festivals, local experiences  
- ğŸ› **Dining**: Vegetarian/halal restaurants, regional cuisines
- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ **Family Travel**: Kid-friendly destinations, safety tips
- ğŸ’° **Budget Planning**: Cost estimates, money-saving tips
- ğŸ“‹ **Documentation**: Visa requirements, travel documents

## ğŸ”„ Advanced Training Options

### Custom Models
```bash
# Use different LLaMA 3 variants
python training/train_llama3b_unsloth.py \
  --model_name "unsloth/llama-3-8b-bnb-4bit"  # Base model

python training/train_llama3b_unsloth.py \
  --model_name "unsloth/llama-3-70b-Instruct-bnb-4bit"  # Larger model
```

### Extended Training
```bash
# Train for more epochs (better quality)
python training/train_llama3b_unsloth.py \
  --model_name "unsloth/llama-3-8b-Instruct-bnb-4bit" \
  --num_train_epochs 3
```

## ğŸ“ Repository Structure

```
ğŸ“¦ llama3-india-travel-assistant/
â”œâ”€â”€ ğŸ“ data/final/
â”‚   â”œâ”€â”€ ğŸ“„ train_llama_format.json (31MB)      # Training dataset
â”‚   â””â”€â”€ ğŸ“„ validation_llama_format.json (3.5MB) # Validation dataset
â”œâ”€â”€ ğŸ“ training/
â”‚   â”œâ”€â”€ ğŸš€ train_llama3b_unsloth.py            # Ultra-fast training script
â”‚   â”œâ”€â”€ âš¡ inference_unsloth.py               # Fast inference script
â”‚   â””â”€â”€ ğŸ“„ requirements.txt                   # Optimized dependencies
â””â”€â”€ ğŸ“„ README.md                              # This guide
```

## â“ Troubleshooting

### Common Issues & Solutions

**ğŸš¨ CUDA Out of Memory**
```bash
# Reduce batch size and sequence length
python training/train_llama3b_unsloth.py \
  --per_device_train_batch_size 1 \
  --max_seq_length 1024
```

**ğŸš¨ Hugging Face Access Denied**
```bash
# Make sure you have LLaMA 3 access and are logged in
huggingface-cli login
# Request access: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
```

**ğŸš¨ Slow Training**
```bash
# Make sure you're using Unsloth optimized models
--model_name "unsloth/llama-3-8b-Instruct-bnb-4bit"  # âœ… Fast
--model_name "meta-llama/Meta-Llama-3-8B-Instruct"   # âŒ Slow
```

## ğŸ“ˆ Expected Results

After training, test with these India-specific queries:

```
ğŸ§ª "Plan a 5-day Kerala backwater trip for â‚¹30,000"
ğŸ§ª "Best vegetarian restaurants in Rajasthan"
ğŸ§ª "How to travel from Delhi to Ladakh by road?"
ğŸ§ª "Visa requirements for Indians visiting Thailand"
ğŸ§ª "Family-friendly hotels in Goa under â‚¹5,000/night"
```

Your model should provide detailed, culturally-aware responses with:
- **Local insights** and cultural preferences
- **Budget considerations** in Indian Rupees
- **Vegetarian/halal** dining options
- **Family-friendly** recommendations
- **Practical advice** for Indian travelers

## ğŸ‰ Success! You've Built an AI Travel Assistant

Your trained model is now ready to help with:
- Personalized India travel planning
- Cultural recommendations  
- Budget-friendly options
- Family travel advice
- International travel for Indians

## ğŸ“„ License

MIT License - Feel free to use for research and commercial purposes.

---

**â­ Star this repo if it helped you build an amazing travel assistant! â­** 